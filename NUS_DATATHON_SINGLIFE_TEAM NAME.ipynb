{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(filepath)\n",
    "\n",
    "# Convert target col to 0 or 1 (binary)\n",
    "df[\"f_purchase_lh\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. General Client Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race_desc        0\n",
       "ctrycode_desc    0\n",
       "cltsex_fix       0\n",
       "stat_flag        0\n",
       "clttype          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imputing missing values for client information\n",
    "clnt_info_columns = ['race_desc', 'ctrycode_desc', 'cltsex_fix', 'clntnum', 'clttype', 'stat_flag', 'min_occ_date', 'cltdob_fix']\n",
    "\n",
    "for clnt_info_column in clnt_info_columns:\n",
    "    df[clnt_info_column] = df[clnt_info_column].fillna(df[clnt_info_column].mode()[0])\n",
    "\n",
    "df[['race_desc', 'ctrycode_desc', 'cltsex_fix', 'stat_flag', 'clttype']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Client Risk and Status Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flg_substandard               0\n",
       "flg_is_borderline_standard    0\n",
       "flg_is_revised_term           0\n",
       "flg_is_rental_flat            0\n",
       "flg_has_health_claim          0\n",
       "flg_has_life_claim            0\n",
       "flg_gi_claim                  0\n",
       "flg_is_proposal               0\n",
       "flg_with_preauthorisation     0\n",
       "flg_is_returned_mail          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imputing missing client risk and status indicator\n",
    "clnt_risk_status_columns = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation', 'flg_is_returned_mail']\n",
    "\n",
    "for clnt_risk_status_column in clnt_risk_status_columns:\n",
    "    df[clnt_risk_status_column] = df[clnt_risk_status_column].fillna(df[clnt_risk_status_column].mode()[0])\n",
    "\n",
    "df[['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation', 'flg_is_returned_mail']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Client Consent and Communication Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_consent_to_mail     0\n",
       "is_consent_to_email    0\n",
       "is_consent_to_call     0\n",
       "is_consent_to_sms      0\n",
       "is_valid_dm            0\n",
       "is_valid_email         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imputing missing client consent and communication preferences\n",
    "clnt_consent_columns = ['is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email']\n",
    "\n",
    "for clnt_consent_column in clnt_consent_columns:\n",
    "    df[clnt_consent_column] = df[clnt_consent_column].fillna(df[clnt_consent_column].mode()[0])\n",
    "\n",
    "df[['is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Policy and Claim History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flg_latest_being_lapse, flg_latest_being_cancel, tot_inforce_pols do not have missing values\n",
    "empty_val_check = df[['flg_latest_being_lapse','flg_latest_being_cancel','tot_inforce_pols']]\n",
    "empty_val_check.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing negative values for n_months_last_bought_products, no missing values\n",
    "median_without_negatives = df['n_months_last_bought_products'][df['n_months_last_bought_products'] >= 0].median()\n",
    "\n",
    "df['n_months_last_bought_products'] = df['n_months_last_bought_products'].apply(lambda x: x if x >= 0 else median_without_negatives)\n",
    "\n",
    "#imputing recency_lapse with 9999 if flg_latest_being_lapse == 0, otherwise median \n",
    "df.loc[df['flg_latest_being_lapse'] == 0, 'recency_lapse'] = 9999\n",
    "df.loc[(df['flg_latest_being_lapse'] == 1) & (df['recency_lapse'].isna()), 'recency_lapse'] = 0\n",
    "df['flg_latest_being_lapse'].isna().sum()\n",
    "\n",
    "#doing the same for cancellations\n",
    "df.loc[df['flg_latest_being_cancel'] == 0, 'recency_cancel'] = 9999\n",
    "df.loc[(df['flg_latest_being_cancel'] == 1) & (df['recency_cancel'].isna()), 'recency_cancel'] = 0\n",
    "df['flg_latest_being_cancel'].isna().sum()\n",
    "\n",
    "#tot_inforce_pols has no missing values\n",
    "\n",
    "#imputing missing values of tot_cancel_pols\n",
    "df.loc[df['flg_latest_being_cancel'] == 0, 'tot_cancel_pols'] = 0\n",
    "df.loc[(df['tot_cancel_pols'].isna()) & (df['flg_latest_being_cancel'] == 1), 'tot_cancel_pols'] = 1\n",
    "\n",
    "#imputing missing values of f_ever_declined_la\n",
    "df['f_ever_declined_la'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Demographic and Household Information, and Other Flags and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data type of the column to the correct data type\n",
    "df['hh_size_est'].replace('>4', 5, inplace=True)\n",
    "df['hh_size_est'] = pd.to_numeric(df['hh_size_est'], errors='coerce')\n",
    "\n",
    "\n",
    "income_mapping = {\n",
    "    'A.ABOVE200K': 200001,  \n",
    "    'B.100K-200K': 150000,  \n",
    "    'C.60K-100K': 80000,\n",
    "    'D.30K-60K': 45000,\n",
    "    'E.BELOW30K': 15000\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'annual_income' column\n",
    "df['annual_income_est'].replace(income_mapping, inplace=True)\n",
    "\n",
    "# Fill in missing values with the median \n",
    "col_names_fill_median = ['hh_20', 'pop_20', 'hh_size', 'hh_size_est', 'annual_income_est']\n",
    "\n",
    "for c in col_names_fill_median:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    median_value = df[c].median()\n",
    "    df[c].fillna(median_value, inplace=True)\n",
    "\n",
    "# Fill in missing values with the mode \n",
    "col_names_fill_mode = ['is_housewife_retiree', 'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy']\n",
    "\n",
    "for c in col_names_fill_mode:\n",
    "    mode_value = df[c].mode()[0]\n",
    "    df[c].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Fill in missing values with 0\n",
    "col_names_fill_zero = ['flg_affconnect_show_interest_ever', 'flg_affconnect_ready_to_buy_ever', 'flg_affconnect_lapse_ever', 'affcon_visit_days', 'clmcon_visit_days']\n",
    "for c in col_names_fill_zero:\n",
    "    df[c].fillna(0, inplace=True)\n",
    "\n",
    "# Fill in missing values with 9999\n",
    "col_names_fill_9999 = ['n_months_since_visit_affcon', 'recency_clmcon', 'recency_clmcon_regis']\n",
    "for c in col_names_fill_9999:\n",
    "    df[c].fillna(9999, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Purchase and Lapse Metrics for Specific Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629600"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking number of empty values in lapse data\n",
    "lapse_specific = df.loc[:, 'lapse_ape_ltc_1280bf':'n_months_since_lapse_32c74c']\n",
    "lapse_specific.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in missing values with 0 for lapses\n",
    "for column in df.loc[:, 'lapse_ape_ltc_1280bf':'lapse_ape_32c74c']:\n",
    "    df[column] = df[column].fillna(0)\n",
    "\n",
    "# Fill in missing values with 9999 for n_months_since_lapse\n",
    "for column in df.loc[:, 'n_months_since_lapse_ltc_1280bf':'n_months_since_lapse_32c74c']:\n",
    "    df[column] = df[column].fillna(9999)\n",
    "\n",
    "df['n_months_since_lapse_ltc_1280bf'][13337] # checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking number of empty values in purchase data\n",
    "purchase_specific = df.loc[:, 'f_ever_bought_839f8a':'n_months_last_bought_32c74c']\n",
    "purchase_specific.isna().sum().sum() # 0 -> no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checking for Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated 'clntnum' values: 0\n"
     ]
    }
   ],
   "source": [
    "duplicated_clntnum_count = df['clntnum'].duplicated(keep=False).sum()\n",
    "print(f\"Number of duplicated 'clntnum' values: {duplicated_clntnum_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Selection by Dropping Low Variance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clntnum                      object\n",
      "race_desc                    object\n",
      "ctrycode_desc                object\n",
      "clttype                      object\n",
      "stat_flag                    object\n",
      "                             ...   \n",
      "flg_gi_claim_29d435_ever     object\n",
      "flg_gi_claim_058815_ever     object\n",
      "flg_gi_claim_42e115_ever     object\n",
      "flg_gi_claim_856320_ever     object\n",
      "f_purchase_lh               float64\n",
      "Length: 304, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clntnum</th>\n",
       "      <th>race_desc</th>\n",
       "      <th>ctrycode_desc</th>\n",
       "      <th>clttype</th>\n",
       "      <th>stat_flag</th>\n",
       "      <th>min_occ_date</th>\n",
       "      <th>cltdob_fix</th>\n",
       "      <th>cltsex_fix</th>\n",
       "      <th>flg_substandard</th>\n",
       "      <th>flg_is_borderline_standard</th>\n",
       "      <th>...</th>\n",
       "      <th>recency_giclaim</th>\n",
       "      <th>giclaim_cnt_success</th>\n",
       "      <th>recency_giclaim_success</th>\n",
       "      <th>giclaim_cnt_unsuccess</th>\n",
       "      <th>recency_giclaim_unsuccess</th>\n",
       "      <th>flg_gi_claim_29d435_ever</th>\n",
       "      <th>flg_gi_claim_058815_ever</th>\n",
       "      <th>flg_gi_claim_42e115_ever</th>\n",
       "      <th>flg_gi_claim_856320_ever</th>\n",
       "      <th>f_purchase_lh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19550</th>\n",
       "      <td>91b546e924</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>1974-05-09</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>896bae548c</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>1979-11-11</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13337</th>\n",
       "      <td>f364439ae6</td>\n",
       "      <td>Others</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>1976-01-28</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15074</th>\n",
       "      <td>70f319cfe1</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2021-10-18</td>\n",
       "      <td>1976-03-19</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19724</th>\n",
       "      <td>2647a81328</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2018-07-20</td>\n",
       "      <td>1995-07-31</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          clntnum race_desc ctrycode_desc clttype stat_flag min_occ_date  \\\n",
       "19550  91b546e924   Chinese     Singapore       P    ACTIVE   2017-10-31   \n",
       "4600   896bae548c   Chinese     Singapore       P    ACTIVE   2007-05-23   \n",
       "13337  f364439ae6    Others     Singapore       P    ACTIVE   2019-08-31   \n",
       "15074  70f319cfe1   Chinese     Singapore       P    ACTIVE   2021-10-18   \n",
       "19724  2647a81328   Chinese     Singapore       P    ACTIVE   2018-07-20   \n",
       "\n",
       "       cltdob_fix cltsex_fix  flg_substandard  flg_is_borderline_standard  \\\n",
       "19550  1974-05-09     Female              0.0                         0.0   \n",
       "4600   1979-11-11       Male              0.0                         0.0   \n",
       "13337  1976-01-28       Male              0.0                         0.0   \n",
       "15074  1976-03-19     Female              0.0                         0.0   \n",
       "19724  1995-07-31     Female              0.0                         0.0   \n",
       "\n",
       "       ...  recency_giclaim  giclaim_cnt_success  recency_giclaim_success  \\\n",
       "19550  ...              NaN                 None                     None   \n",
       "4600   ...              NaN                 None                     None   \n",
       "13337  ...              NaN                 None                     None   \n",
       "15074  ...              NaN                 None                     None   \n",
       "19724  ...              NaN                 None                     None   \n",
       "\n",
       "       giclaim_cnt_unsuccess  recency_giclaim_unsuccess  \\\n",
       "19550                   None                       None   \n",
       "4600                    None                       None   \n",
       "13337                   None                       None   \n",
       "15074                   None                       None   \n",
       "19724                   None                       None   \n",
       "\n",
       "       flg_gi_claim_29d435_ever  flg_gi_claim_058815_ever  \\\n",
       "19550                      None                      None   \n",
       "4600                       None                      None   \n",
       "13337                      None                      None   \n",
       "15074                      None                      None   \n",
       "19724                      None                      None   \n",
       "\n",
       "       flg_gi_claim_42e115_ever  flg_gi_claim_856320_ever  f_purchase_lh  \n",
       "19550                      None                      None            0.0  \n",
       "4600                       None                      None            0.0  \n",
       "13337                      None                      None            0.0  \n",
       "15074                      None                      None            0.0  \n",
       "19724                      None                      None            0.0  \n",
       "\n",
       "[5 rows x 304 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ningy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m         p_values\u001b[38;5;241m.\u001b[39mappend(p_value)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Combine results into a DataFrame for easier analysis\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m correlation_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCorrelation Coefficient\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrelation_coefficients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP-Value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_values\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Sort by absolute correlation coefficient in descending order\u001b[39;00m\n\u001b[0;32m     42\u001b[0m correlation_df \u001b[38;5;241m=\u001b[39m correlation_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrelation Coefficient\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ningy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\ningy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ningy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\ningy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# Define Features (X) and Target Variable (y)\n",
    "X = df.drop('f_purchase_lh', axis=1)  # Features\n",
    "y = df['f_purchase_lh']  # Target variable\n",
    "\n",
    "# Exclude specific columns\n",
    "columns_to_exclude = [\n",
    "    'recency_giclaim', 'giclaim_cnt_success', 'recency_giclaim_success',\n",
    "    'giclaim_cnt_unsuccess', 'recency_giclaim_unsuccess',\n",
    "    'flg_gi_claim_29d435_ever', 'flg_gi_claim_058815_ever',\n",
    "    'flg_gi_claim_42e115_ever', 'flg_gi_claim_856320_ever',\n",
    "    'recency_hlthclaim', 'hlthclaim_cnt_success', 'recency_hlthclaim_success',\n",
    "    'hlthclaim_cnt_unsuccess', 'recency_hlthclaim_unsuccess',\n",
    "    'flg_hlthclaim_839f8a_ever', 'recency_hlthclaim_839f8a',\n",
    "    'flg_hlthclaim_14cb37_ever', 'recency_hlthclaim_14cb37',\n",
    "    'hlthclaim_amt', 'giclaim_amt'\n",
    "]\n",
    "\n",
    "X = X.drop(columns=columns_to_exclude, errors='ignore')\n",
    "\n",
    "# Ensure all arrays have the same length\n",
    "correlation_coefficients, p_values, feature_names = [], [], []\n",
    "\n",
    "for column in X.columns:\n",
    "    if X[column].dtype != 'object':  # Exclude non-numerical columns\n",
    "        correlation_coefficient, p_value = pointbiserialr(X[column], y)\n",
    "        correlation_coefficients.append(abs(correlation_coefficient))\n",
    "        p_values.append(p_value)\n",
    "        feature_names.append(column)\n",
    "\n",
    "# Combine results into a DataFrame for easier analysis\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Correlation Coefficient': correlation_coefficients,\n",
    "    'P-Value': p_values\n",
    "})\n",
    "\n",
    "# Sort by absolute correlation coefficient in descending order\n",
    "correlation_df = correlation_df.sort_values(by='Correlation Coefficient', ascending=False)\n",
    "\n",
    "# Display the correlation results\n",
    "print(\"Point-Biserial Correlation Results:\")\n",
    "print(correlation_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    result = [] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
