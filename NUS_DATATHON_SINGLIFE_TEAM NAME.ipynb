{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning \n",
    "##### Imputing NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datathon.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatathon.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\teomi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    504\u001b[0m     path,\n\u001b[0;32m    505\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    506\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    507\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    509\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\teomi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parquet.py:244\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    242\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    252\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    253\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\teomi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m     92\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\teomi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datathon.parquet'"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('datathon.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m clnt_info_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace_desc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctrycode_desc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcltsex_fix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclntnum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclttype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat_flag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_occ_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcltdob_fix\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clnt_info_column \u001b[38;5;129;01min\u001b[39;00m clnt_info_columns:\n\u001b[1;32m----> 5\u001b[0m     df[clnt_info_column] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[clnt_info_column]\u001b[38;5;241m.\u001b[39mfillna(df[clnt_info_column]\u001b[38;5;241m.\u001b[39mmode()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      7\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace_desc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctrycode_desc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcltsex_fix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstat_flag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclttype\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#imputing missing values for client information\n",
    "clnt_info_columns = ['race_desc', 'ctrycode_desc', 'cltsex_fix', 'clntnum', 'clttype', 'stat_flag', 'min_occ_date', 'cltdob_fix']\n",
    "\n",
    "for clnt_info_column in clnt_info_columns:\n",
    "    df[clnt_info_column] = df[clnt_info_column].fillna(df[clnt_info_column].mode()[0])\n",
    "\n",
    "df[['race_desc', 'ctrycode_desc', 'cltsex_fix', 'stat_flag', 'clttype']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flg_substandard               0\n",
       "flg_is_borderline_standard    0\n",
       "flg_is_revised_term           0\n",
       "flg_is_rental_flat            0\n",
       "flg_has_health_claim          0\n",
       "flg_has_life_claim            0\n",
       "flg_gi_claim                  0\n",
       "flg_is_proposal               0\n",
       "flg_with_preauthorisation     0\n",
       "flg_is_returned_mail          0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imputing missing client risk and status indicator\n",
    "clnt_risk_status_columns = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation', 'flg_is_returned_mail']\n",
    "\n",
    "for clnt_risk_status_column in clnt_risk_status_columns:\n",
    "    df[clnt_risk_status_column] = df[clnt_risk_status_column].fillna(df[clnt_risk_status_column].mode()[0])\n",
    "\n",
    "df[['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation', 'flg_is_returned_mail']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_consent_to_mail     0\n",
       "is_consent_to_email    0\n",
       "is_consent_to_call     0\n",
       "is_consent_to_sms      0\n",
       "is_valid_dm            0\n",
       "is_valid_email         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imputing missing client consent and communication preferences\n",
    "clnt_consent_columns = ['is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email']\n",
    "\n",
    "for clnt_consent_column in clnt_consent_columns:\n",
    "    df[clnt_consent_column] = df[clnt_consent_column].fillna(df[clnt_consent_column].mode()[0])\n",
    "\n",
    "df[['is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing negative values for n_months_last_bought_products, no missing values\n",
    "median_without_negatives = df['n_months_last_bought_products'][df['n_months_last_bought_products'] >= 0].median()\n",
    "\n",
    "df['n_months_last_bought_products'] = df['n_months_last_bought_products'].apply(lambda x: x if x >= 0 else median_without_negatives)\n",
    "\n",
    "#flg_latest_being_lapse & flg_latest_being_cancel do not have missing values\n",
    "\n",
    "#imputing recency_lapse with 9999 if flg_latest_being_lapse == 0, otherwise median \n",
    "df.loc[df['flg_latest_being_lapse'] == 0, 'recency_lapse'] = 9999\n",
    "df.loc[(df['flg_latest_being_lapse'] == 1) & (df['recency_lapse'].isna()), 'recency_lapse'] = 0\n",
    "df['flg_latest_being_lapse'].isna().sum()\n",
    "\n",
    "#doing the same for cancellations\n",
    "df.loc[df['flg_latest_being_cancel'] == 0, 'recency_cancel'] = 9999\n",
    "df.loc[(df['flg_latest_being_cancel'] == 1) & (df['recency_cancel'].isna()), 'recency_cancel'] = 0\n",
    "df['flg_latest_being_cancel'].isna().sum()\n",
    "\n",
    "#tot_inforce_pols has no missing values\n",
    "\n",
    "#imputing missing values of tot_cancel_pols\n",
    "df.loc[df['flg_latest_being_cancel'] == 0, 'tot_cancel_pols'] = 0\n",
    "df.loc[(df['tot_cancel_pols'].isna()) & (df['flg_latest_being_cancel'] == 1), 'tot_cancel_pols'] = 1\n",
    "\n",
    "#imputing missing values of f_ever_declined_la\n",
    "df['f_ever_declined_la'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(filepath)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Convert the data type of the column to the correct data type\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhh_size_est\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(filepath)\n",
    "\n",
    "# Convert the data type of the column to the correct data type\n",
    "df['hh_size_est'].replace('>4', 5, inplace=True)\n",
    "\n",
    "income_mapping = {\n",
    "    'A.ABOVE200K': 200001,  \n",
    "    'B.100K-200K': 150000,  \n",
    "    'C.60K-100K': 80000,\n",
    "    'D.30K-60K': 45000,\n",
    "    'E.BELOW30K': 15000\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'annual_income' column\n",
    "df['annual_income_est'].replace(income_mapping, inplace=True)\n",
    "\n",
    "# Fill in missing values with the median \n",
    "col_names_fill_median = ['hh_20', 'pop_20', 'hh_size', 'hh_size_est', 'annual_income_est']\n",
    "\n",
    "for c in col_names_fill_median:\n",
    "    median_value = df[c].median()\n",
    "    df[c].fillna(median_value, inplace=True)\n",
    "\n",
    "# Fill in missing values with the mode \n",
    "col_names_fill_mode = ['is_housewife_retiree', 'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy']\n",
    "\n",
    "for c in col_names_fill_mode:\n",
    "    mode_value = df[c].mode()[0]\n",
    "    df[c].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Fill in missing values with 0\n",
    "col_names_fill_zero = ['flg_affconnect_show_interest_ever', 'flg_affconnect_ready_to_buy_ever', 'flg_affconnect_lapse_ever', 'affcon_visit_days', 'clmcon_visit_days']\n",
    "for c in col_names_fill_zero:\n",
    "    df[c].fillna(0, inplace=True)\n",
    "\n",
    "# Fill in missing values with 9999\n",
    "col_names_fill_9999 = ['n_months_since_visit_affcon', 'recency_clmcon', 'recency_clmcon_regis']\n",
    "for c in col_names_fill_9999:\n",
    "    df[c].fillna(9999, inplace=True)\n",
    "\n",
    "# Convert target col to 0 or 1 (binary)\n",
    "df[\"f_purchase_lh\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated 'clntnum' values: 0\n"
     ]
    }
   ],
   "source": [
    "duplicated_clntnum_count = df['clntnum'].duplicated(keep=False).sum()\n",
    "print(f\"Number of duplicated 'clntnum' values: {duplicated_clntnum_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    result = [] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
